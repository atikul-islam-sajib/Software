# -*- coding: utf-8 -*-
"""Colon Cancer Classification using CNN & Transfer Learning & Fine Tuning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uBbWXDW0kjn1wg3K8g-CktIsID-UT6lM
"""

##### Import all necessity functions for Machine Learning #####
import sys
import math
import string
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import scipy as shc
import warnings
import zipfile
import cv2
import os
import random
from collections import Counter
from functools import reduce
from itertools import chain
from google.colab.patches import cv2_imshow
from keras.preprocessing import image
from sklearn.metrics._plot.confusion_matrix import confusion_matrix
from sklearn.model_selection import train_test_split, KFold, StratifiedKFold, GridSearchCV, RandomizedSearchCV
from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.feature_selection import mutual_info_classif, mutual_info_regression, SelectKBest, chi2, VarianceThreshold
from imblearn.under_sampling import RandomUnderSampler, NearMiss
from imblearn.over_sampling import RandomOverSampler, SMOTE, SMOTEN, SMOTENC, SVMSMOTE, KMeansSMOTE, BorderlineSMOTE, ADASYN
from imblearn.ensemble import EasyEnsembleClassifier
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB
from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor, NearestNeighbors
from sklearn.linear_model import LinearRegression, LogisticRegression, SGDClassifier, SGDRegressor, Perceptron
from sklearn.neural_network import MLPClassifier, MLPRegressor
from sklearn.svm import SVC, SVR
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, ExtraTreeClassifier, ExtraTreeRegressor
from sklearn.ensemble import BaggingClassifier, BaggingRegressor, RandomForestClassifier, RandomForestRegressor, VotingClassifier, VotingRegressor
from sklearn.ensemble import AdaBoostClassifier, AdaBoostRegressor, GradientBoostingClassifier, GradientBoostingRegressor, StackingClassifier, StackingRegressor
from sklearn.metrics import classification_report, mean_absolute_error, mean_squared_error, r2_score, accuracy_score, recall_score, precision_score, f1_score, silhouette_score
from xgboost import XGBClassifier, XGBRegressor

##### Download keras #####
!pip install keras

##### Import all necessity functions for Neural Network #####
import tensorflow as tf
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.utils import plot_model
from tensorflow.keras.layers import Dense, Conv2D, LSTM, GRU, RNN, Flatten, AvgPool2D, MaxPool2D, GlobalAveragePooling2D, BatchNormalization, Dropout, LeakyReLU, ELU, PReLU
from tensorflow.keras.activations import tanh, relu, sigmoid, softmax, swish
from tensorflow.keras.regularizers import L1, L2, L1L2
from tensorflow.keras.optimizers import SGD, Adagrad, Adadelta, RMSprop, Adam, Adamax, Nadam
from tensorflow.keras.initializers import HeNormal, HeUniform, GlorotNormal, GlorotUniform
from tensorflow.keras.losses import SparseCategoricalCrossentropy, CategoricalCrossentropy, hinge, MSE, MAE, Huber
import keras.utils as image

##### Remove all warnings #####
import warnings
warnings.filterwarnings("ignore")

"""Import the Image dataset from the Google Drive"""

##### To Access the Google Drive #####
def google_drive(parameter = None):
  try:
    from google.colab import drive
    drive.mount('/content/drive',force_remount=True)
  except Exception as e:
    print(e.with_traceback)
  else:
    print('\nGoogle Drive access is done.\n'.title())

google_drive()

def drive(first = None, last = None):
  # nested helper function
  def access():
    from google.colab import drive
    return drive.mount('/content/drive',force_remount=True)

  print(access())

drive()

##### To Unzip the folder #####
def unzip_file(parameter_ = None):
  try:
    link_folder_ = '/content/drive/MyDrive/CNN Dataset/Colon Cancer.zip'
    zip_ref = zipfile.ZipFile(link_folder_, 'r')
    zip_ref.extractall()
    zip_ref.close()
  except Exception as e:
    print(e.with_traceback)
  else:
    print('Upzip is done succesfully'.title())

##### Call the Unzip function #####
unzip_file()

##### Directory of the folder #####
_DIRECTORY  = '/content/Colon Cancer/colon_cancer'
##### Number of categories of the Colon Cancer Dataset #####
_CATEGORIES = ['0_normal', '1_ulcerative_colitis', '2_polyps', '3_esophagitis']
##### Create an empty list named data there all the image array will be stored with their category #####
data = []

##### Make the join with the DIRECTORY & CATEGORIES #####
for each_category in _CATEGORIES:
  ##### Make the each folder path #####
  folder_path_ = os.path.join(_DIRECTORY, each_category)
  ##### Merge & Take dataset by category wise #####
  for image_ in os.listdir(folder_path_):
    image_folder_ = os.path.join(folder_path_, image_)
    ##### Make that image into array size #####
    image_array_  = cv2.imread(image_folder_)
    ##### Make the images into a particular shape #####
    image_array_ = cv2.resize(image_array_, (150, 150))
    ##### Create the target class #####
    target_class_= _CATEGORIES.index(each_category)

    ##### Append the data into 'data' list with their target name #####
    data.append([image_array_, target_class_])

  print('{} category of target class is done'.title().format(each_category.split('_')[1]),'\n')
  print('*'*60)

print('Done with all the categories in the dataset.'.capitalize())

##### Plot some of the Image datasets #####
normal_, ulcerative_colitis, polyps, estophagitis = '/content/Colon Cancer/colon_cancer/0_normal',\
                                                   '/content/Colon Cancer/colon_cancer/1_ulcerative_colitis',\
                                                   '/content/Colon Cancer/colon_cancer/2_polyps',\
                                                  '/content/Colon Cancer/colon_cancer/3_esophagitis'

normal_, ulcerative_colitis, polyps, estophagitis = os.listdir(normal_)[0:3],\
                                                    os.listdir(ulcerative_colitis)[0:3],\
                                                    os.listdir(polyps)[0:3],\
                                                    os.listdir(estophagitis)[0:3]

directoty_ = ['/content/Colon Cancer/colon_cancer/0_normal',\
              '/content/Colon Cancer/colon_cancer/1_ulcerative_colitis',\
              '/content/Colon Cancer/colon_cancer/2_polyps',\
              '/content/Colon Cancer/colon_cancer/3_esophagitis']

image_list_ = []
for index_, subset_image in enumerate([normal_, ulcerative_colitis, polyps, estophagitis]):
  for image_ in subset_image:
    image_list_.append(directoty_[index_]+"/"+image_)

##### Plot the images to analysis #####
_, axs = plt.subplots(4, 3, figsize=(18, 18))
axs = axs.flatten()

for img, ax in zip(image_list_, axs):
  ax.set_title(img.split('_')[-2])
  ax.imshow(cv2.resize(cv2.imread(img), (200, 200)))
plt.show()

##### shuffle the data in order to prevent the biasness #####
data_shuffle = lambda x: random.shuffle(x)
data = (data_shuffle(data))

##### Split the dataset into independent and dependent varibales #####
X, y = [], []

for (data_, target_) in data:
  X.append(data_)
  y.append(target_)

print('Independent and Dependent varibale is done'.capitalize())

##### Convert the independent and dependent varibale into numpy array #####
X_numpy = np.array(X)
y_numpy = np.array(y)

##### Normalized the dataset so that during training the ditribution would remain same #####

X_numpy = (X_numpy/ 255)

print('Converted into Numpy array and normalization is done.'.title())

##### Split the dataset into train and test #####
print('Before splitting the shape of X is = {}'.format(X_numpy.shape),'\n')
print('Before splitting the shape of y is = {}'.format(y_numpy.shape),'\n')
print('*'*120,'\n')

X_train, X_test, y_train, y_test = train_test_split(X_numpy, y_numpy, test_size = 0.25, random_state = 42, shuffle = True)

print('The shape of X train of the dataset is = {}'.format(X_train.shape),'\n')
print('The shape of y train of the dataset is = {}'.format(y_train.shape),'\n')
print('The shape of X test  of the dataset is = {}'.format(X_test.shape),'\n')
print('The shape of y test  of the dataset is = {}'.format(y_test.shape),'\n')

"""Build the own model and evaluate the performance"""

##### Create a sequential model #####
model = Sequential()

##### Create first Convolutional Layer with 64 filters with Stride = 1 #####
model.add(Conv2D(filters = 64, kernel_size = (3, 3), strides = (1, 1), padding = 'valid', \
                 kernel_initializer = 'he_normal', activation = 'relu', input_shape = X_numpy.shape[1:]))

##### Use the MaxPooling Layer #####
model.add(MaxPool2D(pool_size = (2, 2), strides = (2, 2)))

##### Create second Convolutional Layer with 64 filters with Stride = 1 #####
model.add(Conv2D(filters = 32, kernel_size = (3, 3), strides = (1, 1), padding = 'valid', \
                 kernel_initializer = 'he_normal', activation = 'relu'))

##### Use the MaxPooling Layer #####
model.add(MaxPool2D(pool_size = (2, 2), strides = (2, 2)))


##### Do the flatten Layer #####
model.add(Flatten())


##### Create first fully connected layer with 128 nuerons #####
model.add(Dense(units = 128, activation = 'relu', kernel_initializer = 'he_normal', kernel_regularizer = L2()))

##### Create second hidden layer with 64 neurons #####
model.add(Dense(units = 64, activation = 'relu', kernel_initializer = 'he_normal'))

##### Use the dropout layer with p value = 0.6 #####
model.add(Dropout(0.6))

##### Create the output layer #####
model.add(Dense(units = 4, activation = 'softmax'))

##### Compile the model #####
model.compile(optimizer = 'Adam', loss = SparseCategoricalCrossentropy(), metrics = ['accuracy'])

##### Show the model summary #####
model.summary()

history_ = model.fit(x = X_train, y = y_train, batch_size = 64, epochs = 100, validation_data = (X_test, y_test), verbose = 1)

##### Show the performance of this model #####
predicted_ = model.predict(X_test)

predicted_ = np.argmax(predicted_, axis = 1)
print('\nEvalution of model in Average = Macro.\n\n')
print('The accuracy score of this dataset is  = {} '.format(accuracy_score(predicted_, y_test),'\n'))
print('The precision score of this dataset is = {} '.format(precision_score(predicted_, y_test, average = 'macro'),'\n'))
print('The recall score of this dataset is = {} '.format(recall_score(predicted_, y_test, average = 'macro'),'\n'))
print('The F1 score of this dataset is = {} '.format(f1_score(predicted_, y_test, average = 'macro'),'\n'))

print('*'*120)

print('\nEvalution of model in Average = Micro.\n\n')
print('The accuracy score of this dataset is  = {} '.format(accuracy_score(predicted_, y_test),'\n'))
print('The precision score of this dataset is = {} '.format(precision_score(predicted_, y_test, average = 'micro'),'\n'))
print('The recall score of this dataset is = {} '.format(recall_score(predicted_, y_test, average = 'micro'),'\n'))
print('The F1 score of this dataset is = {} '.format(f1_score(predicted_, y_test, average = 'micro'),'\n'))

##### Classification report of this dataset #####
print(classification_report(predicted_, y_test))

##### Show the confution matrix of this model for evalution #####
print(confusion_matrix(predicted_, y_test))

print('*'*120,'\n\n')

sns.heatmap(confusion_matrix(predicted_, y_test))
plt.show()

##### Plot the validation loss and train loss #####
plt.title('The validation and train loss is given below.')
plt.plot(history_.history['loss'], label = 'train loss')
plt.plot(history_.history['val_loss'], label = 'test loss')
plt.legend()
plt.show()

print('*'*120,'\n')

plt.title('The train accuracy and train accuracy is given below.\n')
plt.plot(history_.history['accuracy'], label = 'train accuracy')
plt.plot(history_.history['val_accuracy'], label = 'test accuracy')
plt.legend()
plt.show()

import keras.utils as image
class colon_cancer:

  def __init__(self, image_):
    self.image = image_

  def prediction(self):
    ##### Load the image #####
    Image_ = image.load_img(self.image, target_size = (150, 150))
    plt.imshow(Image_)
    ##### Convert this image to Numpy array #####
    Image_array_ = image.img_to_array(Image_)
    ##### Normalized the array #####
    Image_array_numpy = (Image_array_/255)
    test_data = np.expand_dims(Image_array_numpy, axis = 0)
    predicted_ =  model.predict(test_data)
    predicted_ = np.argmax(predicted_, axis = 1)
    if predicted_[0] == 0:
      return 'Normal'
    elif predicted_[0] == 1:
      return 'Ulcer'
    elif predicted_[0] == 2:
      return 'Polyps'
    else:
      return 'Esophagitis'

col = colon_cancer('/content/Colon Cancer/colon_cancer/1_ulcerative_colitis/test_ulcer_ (105).jpg')
col.prediction()

"""Use KFold - 5 Cross Validation to evaluate the model and prevent the overfitting"""

##### Declare KFold - 5 Cross Validation #####
KFold_ = KFold(n_splits = 5, random_state = 42, shuffle = True)
##### Declare the performance matrix #####
count_, accuracy_score_, precision_score_, recall_score_, f1_score_, history_list_ = 1, [], [], [], [], []

##### Split the dataset using KFold - 5 #####
for train_index_, test_index_ in KFold_.split(X_numpy):

  print('# of Cross Validation {} is running.\n'.format(count_))

  ##### Split the dataset into train and test #####
  X_train, X_test = X_numpy[train_index_], X_numpy[test_index_]
  y_train, y_test = y_numpy[train_index_], y_numpy[test_index_]

  ##### Train the model #####
  history_ = model.fit(x = X_train, y = y_train, batch_size = 64, epochs = 100, validation_data = (X_test, y_test), verbose = 1)

  ##### Predict the model #####
  predicted_ = model.predict(X_test)
  predicted_ = np.argmax(predicted_, axis = 1)

  print('\nEvalution of model in Average = Macro.\n\n')
  print('The accuracy score of this dataset is  = {} '.format(accuracy_score(predicted_, y_test),'\n'))
  print('The precision score of this dataset is = {} '.format(precision_score(predicted_, y_test, average = 'macro'),'\n'))
  print('The recall score of this dataset is = {} '.format(recall_score(predicted_, y_test, average = 'macro'),'\n'))
  print('The F1 score of this dataset is = {} '.format(f1_score(predicted_, y_test, average = 'macro'),'\n'))

  print('*'*120)

  print('\nEvalution of model in Average = Micro.\n\n')
  print('The accuracy score of this dataset is  = {} '.format(accuracy_score(predicted_, y_test),'\n'))
  print('The precision score of this dataset is = {} '.format(precision_score(predicted_, y_test, average = 'micro'),'\n'))
  print('The recall score of this dataset is = {} '.format(recall_score(predicted_, y_test, average = 'micro'),'\n'))
  print('The F1 score of this dataset is = {} '.format(f1_score(predicted_, y_test, average = 'micro'),'\n'))

  #### Appending the performance matrix #####
  accuracy_score_.append(accuracy_score(predicted_, y_test))
  precision_score_.append(precision_score(predicted_, y_test, average = 'macro'))
  recall_score_.append(recall_score(predicted_, y_test, average = 'macro'))
  f1_score_.append(f1_score(predicted_, y_test, average = 'macro'))

  ##### Increament the count #####
  count_+= 1

##### Functions as parameters and return values ####
def take_matrix(x):
    def avg(y):
        return (reduce(lambda x_, y_: (x_ + y_), x))/y
    return avg

list_ = [accuracy_score_, precision_score_, recall_score_, f1_score_]
for value in list_:
    addition = take_matrix([0.96625, 0.97125, 0.975, 0.9775, 0.9775])
    print('The score of the matrix is {}'.format(addition(5)))
    
##### Plot the performance ##### 
plt.figure(figsize = (12, 6))
plt.plot(accuracy_score_, label = 'accuracy score')
plt.plot(precision_score_, label = 'precision score')
plt.plot(recall_score_, label = 'recall score')
plt.plot(f1_score_, label = 'f1 score')
plt.legend()
plt.show()

##### Check the performance of KFold - 5 #####
print('The list of accuracy score is {}'.format(accuracy_score_),'\n')
print('The list of precision score is {}'.format(precision_score_),'\n')
print('The list of recall score is {}'.format(recall_score_),'\n')
print('The list of f1_score score is {}'.format(f1_score_),'\n')

print('*'*120,'\n')

print('The mean accuracy is  = {}'.format(np.array(accuracy_score_).mean(),'\n'))
print('The mean precision is = {}'.format(np.array(precision_score_).mean(),'\n'))
print('The mean recall is = {}'.format(np.array(recall_score_).mean(),'\n'))
print('The mean f1_score is = {}'.format(np.array(f1_score_).mean(),'\n'))

"""Use Transfer Learning Techine to Evaluate the model performance
1. VGG - 16
2. ResNet 
3. InceptionNet50
"""

##### Call the transfer learning - VGG16 #####
from tensorflow.keras.applications import VGG16

##### call the VGG16 with the imagenet weights #####
VGG16_ = VGG16(include_top = False, weights = 'imagenet', input_shape = X_numpy.shape[1:])

##### Show the summary of VGG16 #####
VGG16_.summary()

##### Set the trainable parameters = False for freezing the weights and bias #####
VGG16_.trainable = False

##### Create the sequential model #####
model = Sequential()

##### Add this VGG16 into the sequential model #####
model.add(VGG16_)

##### Flatten the model for the fully connected layer #####
model.add(Flatten())

##### Create first hidden layer with 512 neurons #####
model.add(Dense(units = 512, activation = 'relu', kernel_initializer = 'he_normal', kernel_regularizer = L2()))

##### Use the Dropout Layer with P value = 0.5 #####
model.add(Dropout(0.5))

##### Create second hidden layer with 128 neurons #####
model.add(Dense(units = 128, activation = 'relu', kernel_initializer = 'he_normal', kernel_regularizer = L2()))

##### Use the Dropout Layer with P value = 0.5 #####
model.add(Dropout(0.6))

##### Create the output layer #####
model.add(Dense(units = 4, activation = 'softmax'))


##### Compile the model and run #####
model.compile(optimizer = Adam(learning_rate = 0.0001), loss = SparseCategoricalCrossentropy(), metrics = ['accuracy'])

##### plot the model summary #####
model.summary()

##### Fit the model and run #####
history_ = model.fit(x = X_train, y = y_train, batch_size = 64, epochs = 20, validation_data = (X_test, y_test), verbose = 1)

##### Show the performance of this model #####
predicted_ = model.predict(X_test)

predicted_ = np.argmax(predicted_, axis = 1)
print('\nEvalution of model in Average = Macro in VGG16.\n\n')
print('The accuracy score of this dataset is  = {} '.format(accuracy_score(predicted_, y_test),'\n'))
print('The precision score of this dataset is = {} '.format(precision_score(predicted_, y_test, average = 'macro'),'\n'))
print('The recall score of this dataset is = {} '.format(recall_score(predicted_, y_test, average = 'macro'),'\n'))
print('The F1 score of this dataset is = {} '.format(f1_score(predicted_, y_test, average = 'macro'),'\n'))

print('*'*120)

print('\nEvalution of model in Average = Micro in VGG16.\n\n')
print('The accuracy score of this dataset is  = {} '.format(accuracy_score(predicted_, y_test),'\n'))
print('The precision score of this dataset is = {} '.format(precision_score(predicted_, y_test, average = 'micro'),'\n'))
print('The recall score of this dataset is = {} '.format(recall_score(predicted_, y_test, average = 'micro'),'\n'))
print('The F1 score of this dataset is = {} '.format(f1_score(predicted_, y_test, average = 'micro'),'\n'))

##### Plot the validation loss and train loss #####
plt.title('The validation and train loss in VGG16 is given below.')
plt.plot(history_.history['loss'], label = 'train loss')
plt.plot(history_.history['val_loss'], label = 'test loss')
plt.legend()
plt.show()

print('*'*120,'\n')

plt.title('The train accuracy and train accuracy in VGG16 is given below.\n')
plt.plot(history_.history['accuracy'], label = 'train accuracy')
plt.plot(history_.history['val_accuracy'], label = 'test accuracy')
plt.legend()
plt.show()

##### Classification report of this dataset #####
print(classification_report(predicted_, y_test))

##### Show the confution matrix of this model for evalution #####
print(confusion_matrix(predicted_, y_test))

print('*'*120,'\n\n')

sns.heatmap(confusion_matrix(predicted_, y_test))
plt.show()

import keras.utils as image
class colon_cancer:

  def __init__(self, image_):
    self.image = image_

  def prediction(self):
    ##### Load the image #####
    Image_ = image.load_img(self.image, target_size = (150, 150))
    plt.imshow(Image_)
    ##### Convert this image to Numpy array #####
    Image_array_ = image.img_to_array(Image_)
    ##### Normalized the array #####
    Image_array_numpy = (Image_array_/255)
    test_data = np.expand_dims(Image_array_numpy, axis = 0)
    predicted_ =  model.predict(test_data)
    predicted_ = np.argmax(predicted_, axis = 1)
    if predicted_[0] == 0:
      return 'Normal'
    elif predicted_[0] == 1:
      return 'Ulcer'
    elif predicted_[0] == 2:
      return 'Polyps'
    else:
      return 'Esophagitis'

col = colon_cancer('/content/Colon Cancer/colon_cancer/2_polyps/test_polyps_ (24).jpg')
col.prediction()

"""Use ResNet """

##### Import the ResNet50 #####
from tensorflow.keras.applications import ResNet50
##### Call the ResNet50 #####
ResNet_ = ResNet50(weights = 'imagenet', include_top = False, classes = 4, input_shape = X_numpy.shape[1:])

##### Set the trainable parameter False #####
ResNet_.trainable = False

##### Plot the summary #####
ResNet_.summary()

##### Create the sequential model #####
model = Sequential()

##### Add this VGG16 into the sequential model #####
model.add(ResNet_)

##### Flatten the model for the fully connected layer #####
model.add(Flatten())

##### Create first hidden layer with 512 neurons #####
model.add(Dense(units = 256, activation = 'relu', kernel_initializer = 'he_normal', kernel_regularizer = L2()))

##### Create second hidden layer with 128 neurons #####
model.add(Dense(units = 128, activation = 'relu', kernel_initializer = 'he_normal', kernel_regularizer = L2()))

##### Create the output layer #####
model.add(Dense(units = 4, activation = 'softmax'))


##### Compile the model and run #####
model.compile(optimizer = Adam(learning_rate = 0.0001), loss = SparseCategoricalCrossentropy(), metrics = ['accuracy'])

##### plot the model summary #####
model.summary()

##### Fit the model and run #####
history_ = model.fit(x = X_train, y = y_train, batch_size = 64, epochs = 50, validation_data = (X_test, y_test), verbose = 1)

##### Show the performance of this model #####
predicted_ = model.predict(X_test)

predicted_ = np.argmax(predicted_, axis = 1)
print('\nEvalution of model in Average = Macro in ResNet.\n\n')
print('The accuracy score of this dataset is  = {} '.format(accuracy_score(predicted_, y_test),'\n'))
print('The precision score of this dataset is = {} '.format(precision_score(predicted_, y_test, average = 'macro'),'\n'))
print('The recall score of this dataset is = {} '.format(recall_score(predicted_, y_test, average = 'macro'),'\n'))
print('The F1 score of this dataset is = {} '.format(f1_score(predicted_, y_test, average = 'macro'),'\n'))

print('*'*120)

print('\nEvalution of model in Average = Micro in ResNet.\n\n')
print('The accuracy score of this dataset is  = {} '.format(accuracy_score(predicted_, y_test),'\n'))
print('The precision score of this dataset is = {} '.format(precision_score(predicted_, y_test, average = 'micro'),'\n'))
print('The recall score of this dataset is = {} '.format(recall_score(predicted_, y_test, average = 'micro'),'\n'))
print('The F1 score of this dataset is = {} '.format(f1_score(predicted_, y_test, average = 'micro'),'\n'))

##### Plot the validation loss and train loss #####
plt.title('The validation and train loss in ResNet is given below.')
plt.plot(history_.history['loss'], label = 'train loss')
plt.plot(history_.history['val_loss'], label = 'test loss')
plt.legend()
plt.show()

print('*'*120,'\n')

plt.title('The train accuracy and train accuracy in ResNet is given below.\n')
plt.plot(history_.history['accuracy'], label = 'train accuracy')
plt.plot(history_.history['val_accuracy'], label = 'test accuracy')
plt.legend()
plt.show()

"""Use InceptionNet to evaluate the performance"""

##### Import the InceptionResNet from the keras #####
from tensorflow.keras.applications import InceptionResNetV2
InceptionNet_ = InceptionResNetV2(include_top = False, weights = 'imagenet', classes = 4, input_shape = X_numpy.shape[1:])

##### Set the trainable parameters = False #####
InceptionNet_.trainable = False

##### Plot the summary #####
InceptionNet_.summary()

##### Create the sequential model #####
model = Sequential()

##### Add this InceptionNetV2 into the sequential model #####
model.add(InceptionNet_)

##### Flatten the model for the fully connected layer #####
model.add(Flatten())

##### Create first hidden layer with 512 neurons #####
model.add(Dense(units = 512, activation = 'relu', kernel_initializer = 'he_normal', kernel_regularizer = L2()))

##### Create second hidden layer with 256 neurons #####
model.add(Dense(units = 256, activation = 'relu', kernel_initializer = 'he_normal', kernel_regularizer = L2()))

##### Use the Dropout Layer with p = 0.5 #####
model.add(Dropout(0.5))

##### Create the output layer #####
model.add(Dense(units = 4, activation = 'softmax'))


##### Compile the model and run #####
model.compile(optimizer = Adam(learning_rate = 0.0001), loss = SparseCategoricalCrossentropy(), metrics = ['accuracy'])

##### plot the model summary #####
model.summary()

##### Fit the model and run #####
history_ = model.fit(x = X_train, y = y_train, batch_size = 64, epochs = 50, validation_data = (X_test, y_test), verbose = 1)

##### Show the performance of this model #####
predicted_ = model.predict(X_test)

predicted_ = np.argmax(predicted_, axis = 1)
print('\nEvalution of model in Average = Macro in InceptionNetV2.\n\n')
print('The accuracy score of this dataset is  = {} '.format(accuracy_score(predicted_, y_test),'\n'))
print('The precision score of this dataset is = {} '.format(precision_score(predicted_, y_test, average = 'macro'),'\n'))
print('The recall score of this dataset is = {} '.format(recall_score(predicted_, y_test, average = 'macro'),'\n'))
print('The F1 score of this dataset is = {} '.format(f1_score(predicted_, y_test, average = 'macro'),'\n'))

print('*'*120)

print('\nEvalution of model in Average = Micro in InceptionNetV2.\n\n')
print('The accuracy score of this dataset is  = {} '.format(accuracy_score(predicted_, y_test),'\n'))
print('The precision score of this dataset is = {} '.format(precision_score(predicted_, y_test, average = 'micro'),'\n'))
print('The recall score of this dataset is = {} '.format(recall_score(predicted_, y_test, average = 'micro'),'\n'))
print('The F1 score of this dataset is = {} '.format(f1_score(predicted_, y_test, average = 'micro'),'\n'))

##### Show the confusion matrix of this InceptionResNetV2 #####
print('The confusion matrix of this InceptionResNetV2 is given below.\n')
print(confusion_matrix(predicted_, y_test))

##### Show the classification report of this InceptionResNetV2 #####
print('The classification report of this InceptionResNetV2 is given below.\n')
print(classification_report(predicted_, y_test))

##### Plot the validation loss and train loss #####
plt.title('The validation and train loss in InceptionNetV2 is given below.')
plt.plot(history_.history['loss'], label = 'train loss')
plt.plot(history_.history['val_loss'], label = 'test loss')
plt.legend()
plt.show()

print('*'*120,'\n')

plt.title('The train accuracy and train accuracy in InceptionNetV2 is given below.\n')
plt.plot(history_.history['accuracy'], label = 'train accuracy')
plt.plot(history_.history['val_accuracy'], label = 'test accuracy')
plt.legend()
plt.show()

##### Comments after analysis #####
print(
'''
VGG16, InceptionNetV2 and my own model works very well with this dataset. The model that I will build that would be combination of my Own Model and InceptionResNetV2.
''')

"""Use InceptionResNetV2 to build our model to predict that Colon Cancer for our Software Engineering Project."""

##### Split the dataset into train and test again with test_size = 0.30 #####
X_train, X_test, y_train, y_test = train_test_split(X_numpy, y_numpy, test_size = 0.30, random_state = 42, shuffle = True)

print('The shape of X train of the dataset is = {}'.format(X_train.shape),'\n')
print('The shape of y train of the dataset is = {}'.format(y_train.shape),'\n')
print('The shape of X test  of the dataset is = {}'.format(X_test.shape),'\n')
print('The shape of y test  of the dataset is = {}'.format(y_test.shape),'\n')

##### Call the InceptionRestNetV2 object with the desired parameters #####
try:
  InceptionResNetV2_ = InceptionResNetV2(include_top = False,\
                                        weights     = 'imagenet',\
                                        input_shape = (X_numpy.shape[1:]),\
                                        classes     = 4)
except Exception as e:
  print(e.with_traceback)
else:
  print('InceptionResNetV2 object downloaded is done successfuly.\n\n')

  ##### Make the trainable parameters = False as we will use their imagenet weights to train our model #####
  InceptionResNetV2_.trainable = False

  ##### Show the InceptionResNetV2 summary #####
  InceptionResNetV2_.summary()

##### Plot this model summary in a diagram #####
plot_model(InceptionResNetV2_)

try:
  ##### Create the Sequential Model #####
  model = Sequential()

  ##### InceptionResNetV2 objects should be added into the Sequential model for training #####
  model.add(InceptionResNetV2_)

  ##### Do the Flatten the layer for applying Fully Connected Layer #####
  model.add(Flatten())

  ##### Create first hidden layer with 128 neurons with L2() regularization #####
  model.add(Dense(units = 128, activation = 'relu', kernel_initializer = 'he_normal', kernel_regularizer = L2()))

  ##### Use the BatchNormalization Technique to Normalised their distribution #####
  model.add(BatchNormalization())

  ##### Create second hidden layer with 64 neurons with L2() regularization #####
  model.add(Dense(units = 128, activation = 'relu', kernel_initializer = 'he_normal', kernel_regularizer = L2()))

  ##### Use the Dropout layer with p = 0.6 #####
  model.add(Dropout(rate = 0.5))

  ##### Create the output layer with softmax #####
  model.add(Dense(units = 4, activation = 'softmax'))

  ##### Compile the model and check #####
  model.compile(optimizer = Adam(learning_rate = 0.0001), loss = SparseCategoricalCrossentropy(), metrics = ['accuracy'])

except Exception as e:
  print(e.with_traceback)

else:
  ##### Plot the model summary #####
  model.summary()

##### Fit the model of InceptionNetV2 and check the performance #####
history_ = model.fit(x = X_train, y = y_train, batch_size = 128, epochs = 20, validation_data = (X_test, y_test), verbose = 1)

from colon_model import pipeline
from coln_model.predict import prediction
from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score

def test_make_predict(test_data):

    trans, identity = test_data

    results = prediction(transaction = trans,
                              identity = identity)

    assert len(results["predictions"]) == len(trans)


def test_prediction_quality_against_benchmark(pipeline):

    X_train, X_test, y_train, y_test = pipeline
    benchmark = 0.90

    predict_ = pipeline.colon_cancer.fit(X_train, y_train)

    predictions = predict_.predict_proba(X_test)
    score = roc_auc_score(y_test, predictions[:, 1])
    score1 = f1_score(y_test, predictions[:, 1])
    score2 = recall_score(y_test, predictions[:, 1])
    score3 = precision_score(y_test, predictions[:, 1])

    assert predictions is not None
    assert score > benchmark
    assert score1 > benchmark
    assert score2 > benchmark
    assert score3 > benchmark

predicted_ = test_make_predict(y)

predicted_ = np.argmax(predicted_, axis = 1)
print('\nEvalution of model in Average = Macro in InceptionNetV2.\n\n')
print('The accuracy score of this dataset is  = {} '.format(accuracy_score(predicted_, y_test),'\n'))
print('The precision score of this dataset is = {} '.format(precision_score(predicted_, y_test, average = 'macro'),'\n'))
print('The recall score of this dataset is = {} '.format(recall_score(predicted_, y_test, average = 'macro'),'\n'))
print('The F1 score of this dataset is = {} '.format(f1_score(predicted_, y_test, average = 'macro'),'\n'))

print('*'*120)

print('\nEvalution of model in Average = Micro in InceptionNetV2.\n\n')
print('The accuracy score of this dataset is  = {} '.format(accuracy_score(predicted_, y_test),'\n'))
print('The precision score of this dataset is = {} '.format(precision_score(predicted_, y_test, average = 'micro'),'\n'))
print('The recall score of this dataset is = {} '.format(recall_score(predicted_, y_test, average = 'micro'),'\n'))
print('The F1 score of this dataset is = {} '.format(f1_score(predicted_, y_test, average = 'micro'),'\n'))

def sort_evlution_matrix(list_):
  return sorted(list_, reverse =  True)

model_ = {'vanilla': [95, 94.21, 96.87, 96.31], 'VGG16': [97.09, 92.21, 91.84, 92.71], 'ResNet50': [98.08, 93.71, 99.17, 96.51]}
for _, values in model_.items():
  print(max(sort_evlution_matrix(values)))


#### Comments ####
  """
  So, we will choose vanilla model for our dataset.
  """

##### Only Final data structures #####
##### Python does not support final, and the constant of python is declare as captial letter #####
PI = 3.1416

##### The use of higher-order functions #####
from functools import reduce
model_ = {'vanilla': [95, 94.21, 96.87, 96.31], 'VGG16': [97.09, 92.21, 91.84, 92.71], 'ResNet50': [98.08, 93.71, 99.17, 96.51]}

##### Dict has key and value pair #####
for key, value in model_.items():
  print('{} model has avg accuracy is = {}'.format(key, reduce(lambda x, y: x + y, value)/4))

##### Show the confusion matrix of this InceptionResNetV2 #####
print('The confusion matrix of this InceptionResNetV2 is given below.\n')
print(confusion_matrix(predicted_, y_test))

##### Show the classification report of this InceptionResNetV2 #####
print('The classification report of this InceptionResNetV2 is given below.\n')
print(classification_report(predicted_, y_test))

##### Plot the validation loss and train loss #####
plt.title('The validation and train loss in InceptionNetV2 is given below.')
plt.plot(history_.history['loss'], label = 'train loss')
plt.plot(history_.history['val_loss'], label = 'test loss')
plt.legend()
plt.show()

print('*'*60,'\n')

plt.title('The train accuracy and train accuracy in InceptionNetV2 is given below.\n')
plt.plot(history_.history['accuracy'], label = 'train accuracy')
plt.plot(history_.history['val_accuracy'], label = 'test accuracy')
plt.legend()
plt.show()

"""Build the model for single user input for our Project"""

class colon_cancer_project:

  def __init__(self, image_):
    self.image  = image_
    self.figure = plt.figure(figsize = (12, 12))

  def prediction(self):
    ##### Load the image #####
    Image_ = image.load_img(self.image, target_size = (150, 150))
    ax1 = self.figure.add_subplot(1, 2, 1)
    ax1.set_title('The exact image problem is {}'.format(self.image.split('_')[3]).title())
    ax1.imshow(Image_)
    ##### Convert this image to Numpy array #####
    Image_array_ = image.img_to_array(Image_)
    ##### Normalized the array #####
    Image_array_numpy = (Image_array_/255)
    test_data = np.expand_dims(Image_array_numpy, axis = 0)
    predicted_ =  model.predict(test_data)
    predicted_ = np.argmax(predicted_, axis = 1)
    if predicted_[0] == 0:
      return 'Normal'
    elif predicted_[0] == 1:
      return 'Ulcer'
    elif predicted_[0] == 2:
      return 'Polyps'
    else:
      return 'Esophagitis'

cancer_object = colon_cancer_project('/content/Colon Cancer/colon_cancer/2_polyps/test_polyps_ (24).jpg')
result = cancer_object.prediction()
figure_ = plt.figure(figsize = (12, 12))
ax2 = figure_.add_subplot(1, 2, 2)
ax2.set_title('The Prediction from the image is {}'.format(result))
ax2.imshow(cv2.resize(cv2.imread('/content/Colon Cancer/colon_cancer/2_polyps/test_polyps_ (24).jpg'), (150, 150)))
plt.show()

##### Save the model for further use #####
try:
  model.save('colon_cancer.h5')
except Exception as e:
  print(e.with_traceback)
else:
  print('Model is saved successfully.'.capitalize())
